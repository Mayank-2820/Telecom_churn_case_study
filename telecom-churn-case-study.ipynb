{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import all libraries required \nimport pandas as pd\nimport numpy as np\nimport warnings \nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nimport sklearn.preprocessing\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read churn dataset\ntchurn = pd.read_csv(\"../input/telecom_churn_data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets take a look on data \ntchurn.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"There are total %d columns.\" %tchurn.shape[1])\nprint(\"There are total %d observations.\" %tchurn.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets look more into attributes and stats of dataset\ntchurn.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tchurn.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets get all the column names\nfor col in tchurn.columns:\n    print(col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Cleaning:","metadata":{}},{"cell_type":"code","source":"#As we can see last 4 columns of above dataset has month name as part of their name lets make it \n#similar to other column standard\ntchurn = tchurn.rename(columns={'jun_vbc_3g': 'vbc_3g_6', 'jul_vbc_3g': 'vbc_3g_7', 'aug_vbc_3g': 'vbc_3g_8', 'sep_vbc_3g': 'vbc_3g_9'})\n#lets get all the column names\nfor col in tchurn.columns:\n    print(col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets get the all null values of all columns in percentage, it would be better to look in terms of percentage\nprint(\"Total Null Values in percentage:\\n\")\n(100*(tchurn.isnull().sum())/len(tchurn.index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Lets first extract the high value customers before we perform anyother operations on data due to it's huge volume.\n2. High value customers will be closed to 29.9K rows which will take lesser time in computation as compare to whole data.\n3. Filter high-value customers:As mentioned, you need to predict churn only for the high-value customers. Define high-value customers as follows: Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).","metadata":{}},{"cell_type":"code","source":"#Lets look into columns which are important to identify high value customers\n#check if they have null values\ntchurn[['total_rech_amt_7','total_rech_amt_6','av_rech_amt_data_6','av_rech_amt_data_7','total_rech_data_6','total_rech_data_7']].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets impute missing values with '0' to extract high value customers for these columns \ntchurn[['av_rech_amt_data_6','av_rech_amt_data_7','av_rech_amt_data_8','av_rech_amt_data_9','total_rech_data_6','total_rech_data_7','total_rech_data_8','total_rech_data_9']]=tchurn[['av_rech_amt_data_6','av_rech_amt_data_7','av_rech_amt_data_8','av_rech_amt_data_9','total_rech_data_6','total_rech_data_7','total_rech_data_8','total_rech_data_9']].fillna(0, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets impute all these columns with '0' as they look important for model building\ncol4 = ['max_rech_data_6','max_rech_data_7','max_rech_data_8','count_rech_2g_6','count_rech_2g_7','count_rech_2g_8','count_rech_3g_6','count_rech_3g_7','count_rech_3g_8','arpu_3g_6','arpu_3g_7','arpu_3g_8','arpu_2g_6','arpu_2g_7','arpu_2g_8','night_pck_user_6','night_pck_user_7','night_pck_user_8','fb_user_6','fb_user_7','fb_user_8']\ntchurn[col4]=tchurn[col4].replace(np.nan, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets check for null values\ntchurn.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Lets create new fetures for calculating average recharge amount \n##### Drop features which do not add any value.\n##### Impute columns with '0' if they are important for further analysis else drop","metadata":{}},{"cell_type":"code","source":"#lets sum up all types of data recharge in the month\ntchurn['total_rech_num_data_6'] = (tchurn['count_rech_2g_6']+tchurn['count_rech_3g_6']).astype(int)\ntchurn['total_rech_num_data_7'] = (tchurn['count_rech_2g_7']+tchurn['count_rech_3g_7']).astype(int)\ntchurn['total_rech_num_data_8'] = (tchurn['count_rech_2g_8']+tchurn['count_rech_3g_8']).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets calculate total amount spent on recharging data(mobile internet) in the month\n#multiply amount with number of times it was recharged for data \ntchurn['total_rech_amt_data_6'] = tchurn['total_rech_num_data_6']*tchurn['av_rech_amt_data_6']\ntchurn['total_rech_amt_data_7'] = tchurn['total_rech_num_data_7']*tchurn['av_rech_amt_data_7']\ntchurn['total_rech_amt_data_8'] = tchurn['total_rech_num_data_8']*tchurn['av_rech_amt_data_8']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets calculate total monthly recharge for data and call, so sum amounts spents on call and data recharge for the month.\ntchurn['total_month_rech_6'] = tchurn['total_rech_amt_6']+tchurn['total_rech_amt_data_6']\ntchurn['total_month_rech_7'] = tchurn['total_rech_amt_7']+tchurn['total_rech_amt_data_7']\ntchurn['total_month_rech_8'] = tchurn['total_rech_amt_8']+tchurn['total_rech_amt_data_8']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### High Value Customer Extraction","metadata":{}},{"cell_type":"code","source":"#lets extract high value customers based on the average recharge amount in the first two months(6,7) (the good phase).\nhv_cust=tchurn[tchurn[['total_month_rech_6','total_month_rech_7']].mean(axis=1)> tchurn[['total_month_rech_6','total_month_rech_7']].mean(axis=1).quantile(0.7)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets get the number of features and observations in new dataset high value customers\n#hv_cust.info()\nprint(\"There are total %d features.\" %hv_cust.shape[1])\nprint(\"There are total %d observations.\" %hv_cust.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As asked in problem statement, after picking high values customers we have got 29.9K observations. Lets proceed further.","metadata":{}},{"cell_type":"code","source":"#lets get the all null values of all columns in percentage, it would be better to look in terms of percentage\nprint(\"Total Null Values in percentage:\\n\")\n(100*(hv_cust.isnull().sum())/len(hv_cust.index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets define a function  to find all the columns where more than percentahe of values are null.\n#Looking at the above statistics there are many columns where values are null for more than 49% \ndef nullvalue(cutoff):\n    null = (100*(hv_cust.isnull().sum())/len(hv_cust.index))\n    print(\"{} features have more than {}% null values\".format(len(null.loc[null > cutoff]),cutoff))\n    return null.loc[null > cutoff]\nnullvalue(49)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tag churners and remove attributes of the churn phase","metadata":{}},{"cell_type":"markdown","source":"Now tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. \nThe attributes you need to use to tag churners are:\n\ntotal_ic_mou_9\n\ntotal_og_mou_9\n\nvol_2g_mb_9\n\nvol_3g_mb_9\n\nAfter tagging churners, remove all the attributes corresponding to the churn phase (all attributes having ‘ _9’, etc. in their names).","metadata":{}},{"cell_type":"code","source":"col1 = ['vol_3g_mb_9', 'vol_2g_mb_9','total_ic_mou_9','total_og_mou_9']\nhv_cust['churn']=hv_cust[col1].apply(lambda x: 1 if ((x['vol_3g_mb_9']==0) & (x['vol_2g_mb_9']==0.0) & (x['total_ic_mou_9']==0)  & (x['total_og_mou_9']==0)) else 0, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total number of customers churned is:\",len(hv_cust[hv_cust['churn']==1]))\nprint(\"Total number of customers non-churned is:\",len(hv_cust[hv_cust['churn']==0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets take a look on stats\nhv_cust.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Above details explain that there are few rows for churn as compare to non-churn.\n2. Data is imbalanced so need to be very careful before dropping any rows.\n3. We will execute smote to make data balanced before performing PCA.","metadata":{}},{"cell_type":"code","source":"#After tagging churners, remove all the attributes corresponding to the churn phase\n#(all attributes having ‘ _9’, etc. in their names).\nimport re\n#filter all columns where last char in column name is _9\ncol2 = hv_cust.filter(regex=('_9')).columns\n#drop these columns as mentioned\nhv_cust.drop(col2,axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets get the number of features and observations in new dataset high value customers\n#hv_cust.info()\nprint(\"Total features.\",hv_cust.shape[1])\nprint(\"Total observations.\",hv_cust.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are left with 181 features. Lets proceed further and create some new features.","metadata":{}},{"cell_type":"code","source":"#Lets look into few features. Circle id and mobile number can be dropped from the list.\n# circle id has only one value so drop it. mobile number has not much importance in our analysis\nhv_cust.circle_id.value_counts()\nhv_cust.drop(['circle_id','mobile_number'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets look into all date columns and convert them into correct format\n#filter column names where they have date in their name\ncol3 = hv_cust.filter(regex=('date')).columns\ncol3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets Convert dtype of date columns to datetime\nhv_cust['last_date_of_month_6'] = pd.to_datetime(hv_cust['last_date_of_month_6'], format='%m/%d/%Y')\nhv_cust['last_date_of_month_7'] = pd.to_datetime(hv_cust['last_date_of_month_7'], format='%m/%d/%Y')\nhv_cust['last_date_of_month_8'] = pd.to_datetime(hv_cust['last_date_of_month_8'], format='%m/%d/%Y')\nhv_cust['date_of_last_rech_6'] = pd.to_datetime(hv_cust['date_of_last_rech_6'], format='%m/%d/%Y')\nhv_cust['date_of_last_rech_7'] = pd.to_datetime(hv_cust['date_of_last_rech_7'], format='%m/%d/%Y')\nhv_cust['date_of_last_rech_8'] = pd.to_datetime(hv_cust['date_of_last_rech_8'], format='%m/%d/%Y')\nhv_cust['date_of_last_rech_data_6'] = pd.to_datetime(hv_cust['date_of_last_rech_data_6'], format='%m/%d/%Y')\nhv_cust['date_of_last_rech_data_7'] = pd.to_datetime(hv_cust['date_of_last_rech_data_7'], format='%m/%d/%Y')\nhv_cust['date_of_last_rech_data_8'] = pd.to_datetime(hv_cust['date_of_last_rech_data_8'], format='%m/%d/%Y')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets get columns which have more than 0% missing values\nnullvalue(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets look into columns which have only values as 0 as we looked into stats thru describe\n#looks like all 3 columns have only 0 and null values.\nprint(hv_cust['loc_og_t2o_mou'].unique())\nprint(hv_cust['std_og_t2o_mou'].unique())\nprint(hv_cust['loc_ic_t2o_mou'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets drop above 3 columns from dataset\nhv_cust.drop(['loc_og_t2o_mou','std_og_t2o_mou','loc_ic_t2o_mou'],inplace=True,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets look into columns which have only values as 0. \n#looks like all 6 columns have only 0 and null values.\nprint(hv_cust['std_og_t2c_mou_6'].unique())\nprint(hv_cust['std_og_t2c_mou_7'].unique())\nprint(hv_cust['std_og_t2c_mou_8'].unique())\nprint(hv_cust['std_ic_t2o_mou_6'].unique())\nprint(hv_cust['std_ic_t2o_mou_7'].unique())\nprint(hv_cust['std_ic_t2o_mou_8'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets drop above 6 columns from dataset\nhv_cust.drop(['std_og_t2c_mou_6','std_og_t2c_mou_7','std_og_t2c_mou_8','std_ic_t2o_mou_6','std_ic_t2o_mou_7','std_ic_t2o_mou_8'],inplace=True,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets get columns which have more than 3% missing values\nnullvalue(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets drop these columns 3 date columns which have more than 40% values as null.\n#they don't see to much imporatnt as we already have date columns \nhv_cust.drop(['date_of_last_rech_data_6','date_of_last_rech_data_7','date_of_last_rech_data_8'],inplace=True,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Above details show that all these columns belogn to 8th month and have same percentage of null values.\n2. Lets impute these columns with zeros.","metadata":{}},{"cell_type":"code","source":"missing3 = list(nullvalue(3).index)\nmissing3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets impute all these columns with '0' as they look important for model building\nhv_cust[missing3]=hv_cust[missing3].replace(np.nan, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets look into date columns for unique values.\nhv_cust['date_of_last_rech_6'].unique()\nhv_cust['date_of_last_rech_7'].unique()\nhv_cust['date_of_last_rech_8'].unique()\n# they all have dates for only one month in all their rows. that means rechrge was done in that particular month.\n# we will just impute a particular date of that month for all those null value rows.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filling null values with the previous ones\nhv_cust['date_of_last_rech_6'].fillna(method ='pad',inplace=True) \nhv_cust['date_of_last_rech_7'].fillna(method ='pad',inplace=True) \nhv_cust['date_of_last_rech_8'].fillna(method ='pad',inplace=True) \n#lets get columns which have more than 1% missing values\nnullvalue(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets look into date columns for unique values.\nprint(hv_cust['last_date_of_month_7'].unique())\nhv_cust['last_date_of_month_8'].unique()\n# they all have same dates for the month in all their rows and null values for few.\n# we will just impute a particular same date of that month for all those null value rows.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filling null values with the previous ones in the dataset\nhv_cust['last_date_of_month_7'].fillna(method ='pad',inplace=True) \nhv_cust['last_date_of_month_8'].fillna(method ='pad',inplace=True) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now we have 54 columns which have null values which have almost same percentage of null values\n#also we can notice that all these columns belong to 6th and 7th month so lets impute these columsn with 0s.\n#lets get all the columns with null values as they are important for our analysis.\nmissing0 = list(nullvalue(0).index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets impute all these columns with '0' as they look important for model building\nhv_cust[missing0]=hv_cust[missing0].replace(np.nan, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets look for null values one last time\nnullvalue(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we have taken care of all null values.\nprint(\"Total features.\",hv_cust.shape[1])\nprint(\"Total observations.\",hv_cust.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets list all the columns currently present in dataframe\nhv_cust.columns.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis(EDA):","metadata":{}},{"cell_type":"code","source":"# Lets see distribution of same fields in each month using box plot.\n# Quantitative Variables\nimport seaborn as sns\nplt.figure(figsize=(15,8),facecolor='b')\nsns.set_style(\"dark\")\n# subplot 1\nplt.subplot(2, 3, 1)\nax = sns.boxplot(hv_cust['roam_og_mou_6'])\nax.set_title('Outgoing roaming Usage mon-6 - Box Plot',fontsize=14,color='w')\n# subplot 2\nplt.subplot(2, 3, 2)\nax = sns.boxplot(hv_cust['roam_og_mou_7'])\nax.set_title('Outgoing roaming Usage mon-7- Box Plot',fontsize=14,color='w')\n# subplot 2\nplt.subplot(2, 3, 3)\nax = sns.boxplot(hv_cust['roam_og_mou_8'])\nax.set_title('Outgoing roaming Usage mon-8- Box Plot',fontsize=14,color='w')\nplt.show()\n\n# Observation: \n# Distribution of roaming usage shows august month usage has reduced for sure. \n# but it should have been increased if customer is happy.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see distribution of same fields in each motnh using box plot.\n# Quantitative Variables\n\nplt.figure(figsize=(15,8),facecolor='b')\nsns.set_style(\"dark\")\n# subplot 1\nplt.subplot(2, 3, 1)\nax = sns.boxplot(hv_cust['total_og_mou_6'])\nax.set_title('total Outgoing Usage mon-6 - Box Plot',fontsize=14,color='w')\n# subplot 2\nplt.subplot(2, 3, 2)\nax = sns.boxplot(hv_cust['total_og_mou_7'])\nax.set_title('total Outgoing Usage mon-7- Box Plot',fontsize=14,color='w')\n# subplot 2\nplt.subplot(2, 3, 3)\nax = sns.boxplot(hv_cust['total_og_mou_8'])\nax.set_title('total Outgoing Usage mon-8- Box Plot',fontsize=14,color='w')\nplt.show()\n\n# Observation: \n# Distribution of total outgoing usage shows august month usage has reduced for sure. \n# but it should have been increased or constant if customer is happy but it doesn't look that way.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see distribution of same fields in each motnh using box plot.\n# Quantitative Variables\n\nplt.figure(figsize=(15,8),facecolor='b')\nsns.set_style(\"dark\")\n# subplot 1\nplt.subplot(2, 3, 1)\nax = sns.boxplot(hv_cust['total_ic_mou_6'])\nax.set_title('total incomig Usage mon-6 - Box Plot',fontsize=14,color='w')\n# subplot 2\nplt.subplot(2, 3, 2)\nax = sns.boxplot(hv_cust['total_ic_mou_7'])\nax.set_title('total incoming Usage mon-7- Box Plot',fontsize=14,color='w')\n# subplot 2\nplt.subplot(2, 3, 3)\nax = sns.boxplot(hv_cust['total_ic_mou_8'])\nax.set_title('total incoming Usage mon-8- Box Plot',fontsize=14,color='w')\nplt.show()\n\n# Observation: \n# Distribution of total incoming usage shows august month usage has got better or constant for sure. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see distribution of same fields in each motnh using box plot.\n# Quantitative Variables\n\nplt.figure(figsize=(15,8),facecolor='b')\nsns.set_style(\"dark\")\n# subplot 1\nplt.subplot(2, 3, 1)\nax = sns.boxplot(hv_cust['last_day_rch_amt_6'])\nax.set_title('Last Recharge amount mon-6 - Box Plot',fontsize=14,color='w')\n# subplot 2\nplt.subplot(2, 3, 2)\nax = sns.boxplot(hv_cust['last_day_rch_amt_7'])\nax.set_title('Last Recharge amount mon-7 - BOx Plot',fontsize=14,color='w')\n# subplot 2\nplt.subplot(2, 3, 3)\nax = sns.boxplot(hv_cust['last_day_rch_amt_8'])\nax.set_title('Last Recharge amount mon-8 - BOx Plot',fontsize=14,color='w')\nplt.show()\n\n# Observation: \n# Distribution of recharge amount in august shows customer has reduced recharge amount for sure. \n# but it should have been increased or constant if customer is happy but it doesn't look that way.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see distribution of same fields in each motnh using box plot.\n# Quantitative Variables\n\nplt.figure(figsize=(15,8),facecolor='b')\nsns.set_style(\"dark\")\n# subplot 1\nplt.subplot(2, 3, 1)\nax = sns.boxplot(hv_cust['total_month_rech_6'])\nax.set_title('Total monthly recharge-6 - Box Plot',fontsize=14,color='w')\n# subplot 2\nplt.subplot(2, 3, 2)\nax = sns.boxplot(hv_cust['total_month_rech_7'])\nax.set_title('Total monthly recharge-7 - BOx Plot',fontsize=14,color='w')\n# subplot 2\nplt.subplot(2, 3, 3)\nax = sns.boxplot(hv_cust['total_month_rech_8'])\nax.set_title('Total monthly recharge-8 - BOx Plot',fontsize=14,color='w')\nplt.show()\n\n# Observation: \n# Distribution of total monthly recharge amount in august shows customer has reduced recharge amount for sure. \n# but it should have been increased or constant if customer is happy but it doesn't look that way.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see distribution of same fields in each motnh using box plot.\n# Quantitative Variables\n\nplt.figure(figsize=(8,4),facecolor='b')\nsns.set_style(\"dark\")\nax = sns.boxplot(hv_cust['aon'])\nax.set_title('Age on Netwrok - Box Plot',fontsize=14,color='w')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations for below analysis are mentioned at last.","metadata":{}},{"cell_type":"code","source":"#sum of total isd MOU per month churn vs Non-Churn\nhv_cust.groupby(['churn'])['isd_og_mou_6','isd_og_mou_7','isd_og_mou_8'].sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean of total 3G usage per month churn vs Non-Churn\nhv_cust.groupby(['churn'])['vol_3g_mb_6','vol_3g_mb_7','vol_3g_mb_8'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean of total 2G usage per month churn vs Non-Churn\nhv_cust.groupby(['churn'])['vol_2g_mb_6','vol_2g_mb_7','vol_2g_mb_8'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean of total std MOU per month churn vs Non-Churn\nhv_cust.groupby(['churn'])['std_og_mou_6','std_og_mou_7','std_og_mou_8'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sum of total special MOU per month churn vs Non-Churn\nhv_cust.groupby(['churn'])['spl_og_mou_6','spl_og_mou_7','spl_og_mou_8'].sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean of total incoming MOU per month churn vs Non-Churn\nhv_cust.groupby(['churn'])['total_ic_mou_6','total_ic_mou_7','total_ic_mou_8'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean of total outgoing MOU per month churn vs Non-Churn\nhv_cust.groupby(['churn'])['total_og_mou_6','total_og_mou_7','total_og_mou_8'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean of total monthly recharge per month churn vs Non-Churn\nhv_cust.groupby(['churn'])['total_rech_amt_6','total_rech_amt_7','total_rech_amt_8'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean of outgoing in roaming usage per month churn vs Non-Churn\nhv_cust.groupby(['churn'])['roam_og_mou_6','roam_og_mou_7','roam_og_mou_8'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean of maximum recharge amount per month churn vs Non-Churn\nhv_cust.groupby(['churn'])['max_rech_amt_6','max_rech_amt_7','max_rech_amt_8'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean of count of total data recharge per month churn vs Non-Churn\nhv_cust.groupby(['churn'])['total_rech_num_data_6','total_rech_num_data_7','total_rech_num_data_8'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean of last recharge amount churn vs Non-Churn\nhv_cust.groupby(['churn'])['last_day_rch_amt_6','last_day_rch_amt_7','last_day_rch_amt_8'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean of local outgoing on same network usage per month churn vs Non-Churn\nhv_cust.groupby(['churn'])['loc_og_t2t_mou_6','loc_og_t2t_mou_7','loc_og_t2t_mou_8'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mean of age on network churn vs Non-Churn\nhv_cust.groupby(['churn'])['aon'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:","metadata":{}},{"cell_type":"markdown","source":"- We can observe below points from above data analysis:\n - incoming calls were reduced in action phase i.e. in august\n - outgoing calls were reduced in action phase i.e. in august\n - total recharge amount reduced in action phase i.e. in august\n - total data recharge reduced in action phase i.e. in august\n - Reuction is not just normal but significant.\n - Those who churned were customers who used services for less days as compared to customers who didn't churn.\n - Overall most of the services that customer was using, reduced to low in action phase 8th month.","metadata":{}},{"cell_type":"markdown","source":"### Lets build some Models on Train data and test it later.\n### Lets first perform PCA on dataset before doing model building.","metadata":{}},{"cell_type":"code","source":"#lets copy the dataframe to another before we do other activities\nhv_custcopy = hv_cust\nprint(hv_custcopy.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hv_custcopy.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hv_custcopy.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets remove aon column \nhv_custcopy.drop(['aon'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets remove datetime columns from dataset else it will give error further\ndatecols = list(hv_custcopy.select_dtypes(include=['datetime']).columns)\nprint(datecols)\nhv_custcopy.drop(datecols, axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets import train test split \nfrom sklearn.model_selection import train_test_split\nX = hv_custcopy.drop(['churn'], axis=1)\ny = hv_custcopy['churn']    \nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Perform MinMax scaler before performing PCA","metadata":{}},{"cell_type":"code","source":"#perform minmax scaling before PCA\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit transform the scaler on train\nX_train = scaler.fit_transform(X_train)\n# transform test using the already fit scaler\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling class imbalance using smote","metadata":{}},{"cell_type":"code","source":"#lets print the stats before sampling\nprint(\"counts of label '1':\",sum(y_train==1))\nprint(\"counts of label '0':\",sum(y_train==0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#perform oversampling using smote\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=12)\nX_train_smo, y_train_smo = sm.fit_sample(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets print stats after smote\nprint(\"counts of label '1':\",sum(y_train_smo==1))\nprint(\"counts of label '0':\",sum(y_train_smo==0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets perform PCA on sampled data. import PCA\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets fit PCA on the train dataset\npca.fit(X_train_smo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca.explained_variance_ratio_[:50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets draw screeplot in between cumulative variance and number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot shows that 35 components are enough to explain close to 95% variance","metadata":{}},{"cell_type":"code","source":"#lets perform incremental PCA for efficiency \nfrom sklearn.decomposition import IncrementalPCA\npca_again = IncrementalPCA(n_components=35)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fit\nX_train_pca = pca_again.fit_transform(X_train_smo)\nX_train_pca.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets create correlation matrix for the principal components\ncorrmat = np.corrcoef(X_train_pca.transpose())\ncorrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\nprint(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)\n#correlations are close to 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying selected components to the test data - 35 components\nX_test_pca = pca_again.transform(X_test)\nX_test_pca.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modeling : Logistic Regression ","metadata":{}},{"cell_type":"code","source":"#import library and fit train model on train data\n#class_weight=\"balanced\":it basically means replicating the smaller class until you have as many samples as in the larger one, \n#but in an implicit way.Though we have already used smote but here we can use this too.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlearner_pca2 = LogisticRegression(class_weight='balanced')\nlearner_pca2.fit(X_train_pca,y_train_smo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict on training set\ndtrain_predictions = learner_pca2.predict(X_train_pca)\ndtrain_predprob = learner_pca2.predict_proba(X_train_pca)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets print some scores\nprint (\"Accuracy :\",metrics.roc_auc_score(y_train_smo, dtrain_predictions))\nprint (\"Recall/Sensitivity :\",metrics.recall_score(y_train_smo, dtrain_predictions))\nprint (\"AUC Score (Train):\",metrics.roc_auc_score(y_train_smo, dtrain_predprob))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets predict on test dataset.\n#print all scores\npred_probs_test = learner_pca2.predict(X_test_pca)\nconfusion = metrics.confusion_matrix(y_test, pred_probs_test)\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\nprint(\"Roc_auc_score :\",(metrics.roc_auc_score(y_test, pred_probs_test)))\nprint('precision score:',(metrics.precision_score(y_test, pred_probs_test)))\nprint('Sensitivity/Recall :',(TP / float(TP+FN)))\nprint('Specificity:',(TN / float(TN+FP)))\nprint('False Positive Rate:',(FP/ float(TN+FP)))\nprint('Positive predictive value:',(TP / float(TP+FP)))\nprint('Negative Predictive value:',(TN / float(TN+ FN)))\nprint(\"Accuracy :\",(metrics.accuracy_score(y_test,pred_probs_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets check with probability cutoff 0.5\ny_train_pred = learner_pca2.predict_proba(X_train_pca)[:,1]\ny_train_pred_final = pd.DataFrame({'Churn':y_train_smo, 'Churn_Prob':y_train_pred})\ny_train_pred_final['Churn_Prob'] = y_train_pred\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets define function for ROC curve\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets draw roc curve\ndraw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the curve above, 0.45 is the optimum point to take it as a cutoff probability.","metadata":{}},{"cell_type":"code","source":"#apply cutoff probability\ny_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.45 else 0)\ny_train_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets predict on train dataset with optimal cutoff probability\ny_train_pred = learner_pca2.predict_proba(X_train_pca)[:,1]\ny_train_pred_final = pd.DataFrame({'Churn':y_train_smo, 'Churn_Prob':y_train_pred})\ny_train_pred_final['Churn_Prob'] = y_train_pred\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.45 else 0)\ny_train_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets find out all scores of train dataset\n#print all scores\nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted)\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\nprint(\"Roc_auc_score :\",(metrics.roc_auc_score(y_train_pred_final.Churn, y_train_pred_final.predicted)))\nprint('precision score:',(metrics.precision_score(y_train_pred_final.Churn, y_train_pred_final.predicted)))\nprint('Sensitivity/Recall :',(TP / float(TP+FN)))\nprint('Specificity:',(TN / float(TN+FP)))\nprint('False Positive Rate:',(FP/ float(TN+FP)))\nprint('Positive predictive value:',(TP / float(TP+FP)))\nprint('Negative Predictive value:',(TN / float(TN+ FN)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets predict on test datset with optimal cutoff obtained earlier\ny_test_pred = learner_pca2.predict_proba(X_test_pca)[:,1]\ny_test_pred_final = pd.DataFrame({'Churn':y_test, 'Churn_Prob':y_test_pred})\ny_test_pred_final['Churn_Prob'] = y_test_pred\ny_test_pred_final['predicted'] = y_test_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.45 else 0)\ny_test_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets find out all scores of test dataset\n#print all scores\nconfusion = metrics.confusion_matrix(y_test_pred_final.Churn, y_test_pred_final.predicted)\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\nprint(\"Roc_auc_score :\",metrics.roc_auc_score(y_test_pred_final.Churn, y_test_pred_final.predicted))\nprint('precision score :',(metrics.precision_score(y_test_pred_final.Churn, y_test_pred_final.predicted)))\nprint('Sensitivity/Recall :',(TP / float(TP+FN)))\nprint('Specificity:',(TN / float(TN+FP)))\nprint('False Positive Rate:',(FP/ float(TN+FP)))\nprint('Positive predictive value:',(TP / float(TP+FP)))\nprint('Negative Predictive value:',(TN / float(TN+ FN)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see above stats with logistics regression on Test Dataset is:\n1. Sensitivity/Recall : 0.8472834067547724\n2. Specificity: 0.7708609271523179\n\nStats On Train dataset was:\n1. Sensitivity/Recall : 0.8716627634660421\n2. Specificity: 0.7788706739526412","metadata":{}},{"cell_type":"markdown","source":"### Lets apply Random forest Modeling ","metadata":{}},{"cell_type":"markdown","source":"### Tuning max_depth","metadata":{}},{"cell_type":"markdown","source":"Let's try to find the optimum values for ```max_depth``` and understand how the value of max_depth impacts the overall accuracy of the ensemble.","metadata":{}},{"cell_type":"code","source":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(10, 30, 5)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                 return_train_score=True)\nrf.fit(X_train_pca, y_train_smo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"max_depth 20 looks good.","metadata":{}},{"cell_type":"markdown","source":"You can see that as we increase the value of max_depth, both train and test scores increase till a point, but after that test score becomes constant. The ensemble tries to overfit as we increase the max_depth.\nThus, controlling the depth of the constituent trees will help reduce overfitting in the forest.","metadata":{}},{"cell_type":"markdown","source":"### Tuning n_estimators","metadata":{}},{"cell_type":"markdown","source":"Let's try to find the optimum values for n_estimators and understand how the value of n_estimators impacts the overall accuracy. Notice that we'll specify an appropriately low value of max_depth, so that the trees do not overfit.\n<br>","metadata":{}},{"cell_type":"code","source":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'n_estimators': range(50, 150, 25)}\n\n# instantiate the model (note we are specifying a max_depth)\nrf = RandomForestClassifier(max_depth=20)\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",return_train_score=True)\nrf.fit(X_train_pca, y_train_smo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting accuracies with n_estimators\nplt.figure()\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"n_estimators 80 looks good.","metadata":{}},{"cell_type":"markdown","source":"### Tuning max_features\n\nLet's see how the model performance varies with ```max_features```, which is the maximum numbre of features considered for splitting at a node.","metadata":{}},{"cell_type":"code","source":"# GridSearchCV to find optimal max_features\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_features': [4, 8, 14, 20, 24]}\n\n# instantiate the model\nrf = RandomForestClassifier(max_depth=20,n_estimators=80)\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",return_train_score=True)\nrf.fit(X_train_pca, y_train_smo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting accuracies with max_features\nplt.figure()\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Selecting max_features = 5","metadata":{}},{"cell_type":"markdown","source":"### Tuning min_samples_leaf","metadata":{}},{"cell_type":"markdown","source":"The hyperparameter **min_samples_leaf** is the minimum number of samples required to be at a leaf node:\n- If int, then consider min_samples_leaf as the minimum number.\n- If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.","metadata":{}},{"cell_type":"code","source":"# GridSearchCV to find optimal min_samples_leaf\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(100, 400, 50)}\n\n# instantiate the model\nrf = RandomForestClassifier(max_depth=20,n_estimators=80,max_features=5)\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",return_train_score=True)\nrf.fit(X_train_pca, y_train_smo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"min_samples_leaf 100 looks good","metadata":{}},{"cell_type":"markdown","source":"### Tuning min_samples_split\n\nLet's now look at the performance of the ensemble as we vary min_samples_split.","metadata":{}},{"cell_type":"code","source":"# GridSearchCV to find optimal min_samples_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(50, 300, 50)}\n\n# instantiate the model\nrf = RandomForestClassifier(max_depth=20,n_estimators=80,max_features=5,min_samples_leaf=100)\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",return_train_score=True)\nrf.fit(X_train_pca, y_train_smo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting accuracies with min_samples_split\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"min_samples_split looks good for 100.","metadata":{}},{"cell_type":"markdown","source":"**Fitting the final model with the best parameters obtained.**","metadata":{}},{"cell_type":"markdown","source":"We can now find the optimal hyperparameters using GridSearchCV.","metadata":{}},{"cell_type":"code","source":"# model with the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=20,\n                             min_samples_leaf=100, \n                             min_samples_split=100,\n                             max_features=5,\n                             n_estimators=80,\n                             random_state=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit\nrf_pca=rfc.fit(X_train_pca,y_train_smo)\n#Predict on training set\nrtrain_predictions = rf_pca.predict(X_train_pca)\nrtrain_predprob = rf_pca.predict_proba(X_train_pca)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets print some scores\nprint (\"Accuracy :\",metrics.roc_auc_score(y_train_smo, rtrain_predictions))\nprint (\"Recall/Sensitivity :\",metrics.recall_score(y_train_smo, rtrain_predictions))\nprint (\"AUC Score (Train):\",metrics.roc_auc_score(y_train_smo, rtrain_predprob))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets predict on test dataset\npred_probs_test = rf_pca.predict(X_test_pca)\nconfusion = metrics.confusion_matrix(y_test, pred_probs_test)\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\nprint(\"Roc_auc_score :\",(metrics.roc_auc_score(y_test, pred_probs_test)))\nprint('precision score:',(metrics.precision_score(y_test, pred_probs_test)))\nprint('Sensitivity/Recall :',(TP / float(TP+FN)))\nprint('Specificity:',(TN / float(TN+FP)))\nprint('False Positive Rate:',(FP/ float(TN+FP)))\nprint('Positive predictive value:',(TP / float(TP+FP)))\nprint('Negative Predictive value:',(TN / float(TN+ FN)))\nprint(\"Accuracy :\",(metrics.accuracy_score(y_test,pred_probs_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets check with probability cutoff 0.5\ny_train_predrf = rf_pca.predict_proba(X_train_pca)[:,1]\ny_train_predrf_final = pd.DataFrame({'Churn':y_train_smo, 'Churn_Prob':y_train_predrf})\ny_train_predrf_final['Churn_Prob'] = y_train_predrf\ny_train_predrf_final['predicted'] = y_train_predrf_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_predrf_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_predrf_final[i]= y_train_predrf_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_predrf_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_predrf_final.Churn, y_train_predrf_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the curve above, 0.45 is the optimum point to take it as a cutoff probability.","metadata":{}},{"cell_type":"code","source":"#apply cutoff probability\ny_train_predrf_final['final_predicted'] = y_train_predrf_final.Churn_Prob.map( lambda x: 1 if x > 0.45 else 0)\n#lets predict on train dataset with optimal cutoff probability\ny_train_predrf = rf_pca.predict_proba(X_train_pca)[:,1]\ny_train_predrf_final = pd.DataFrame({'Churn':y_train_smo, 'Churn_Prob':y_train_predrf})\ny_train_predrf_final['Churn_Prob'] = y_train_predrf\ny_train_predrf_final['predicted'] = y_train_predrf_final.Churn_Prob.map(lambda x: 1 if x > 0.45 else 0)\ny_train_predrf_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets find out all scores of train dataset\nconfusion = metrics.confusion_matrix(y_train_predrf_final.Churn, y_train_predrf_final.predicted)\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\nprint(\"Roc_auc_score :\",(metrics.roc_auc_score(y_train_predrf_final.Churn, y_train_predrf_final.predicted)))\nprint('precision score:',(metrics.precision_score(y_train_predrf_final.Churn, y_train_predrf_final.predicted)))\nprint('Sensitivity/Recall :',(TP / float(TP+FN)))\nprint('Specificity:',(TN / float(TN+FP)))\nprint('False Positive Rate:',(FP/ float(TN+FP)))\nprint('Positive predictive value:',(TP / float(TP+FP)))\nprint('Negative Predictive value:',(TN / float(TN+ FN)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets predict on test datset with optimal cutoff obtained earlier\ny_test_predrf = rf_pca.predict_proba(X_test_pca)[:,1]\ny_test_predrf_final = pd.DataFrame({'Churn':y_test, 'Churn_Prob':y_test_predrf})\ny_test_predrf_final['Churn_Prob'] = y_test_predrf\ny_test_predrf_final['predicted'] = y_test_predrf_final.Churn_Prob.map(lambda x: 1 if x > 0.45 else 0)\ny_test_predrf_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets find out all scores of test dataset\nconfusion = metrics.confusion_matrix(y_test_predrf_final.Churn, y_test_predrf_final.predicted)\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\nprint(\"Roc_auc_score :\",metrics.roc_auc_score(y_test_predrf_final.Churn, y_test_predrf_final.predicted))\nprint('precision score :',(metrics.precision_score(y_test_predrf_final.Churn, y_test_predrf_final.predicted)))\nprint('Sensitivity/Recall :',(TP / float(TP+FN)))\nprint('Specificity:',(TN / float(TN+FP)))\nprint('False Positive Rate:',(FP/ float(TN+FP)))\nprint('Positive predictive value:',(TP / float(TP+FP)))\nprint('Negative Predictive value:',(TN / float(TN+ FN)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train and test Recall score of Random forest:\n\n- Train Sensitivity/Recall:0.892583918813427\n- Test Sensitivity/Recall:0.8002936857562408\n\n#### Train and test Recall score with logistics regression :\n- Train Sensitivity/Recall: 0.8716627634660421\n- Test Sensitivity/Recall : 0.8472834067547724\n\n","metadata":{}},{"cell_type":"markdown","source":"- Recall/Sensitivity score need to be considered in this case study as Telecom company will not like any high value customer to churn so will try to find out all high value customers who may leave in future. Company may willing to even bear burden of few customers who may not churn but will be classified as churn. \n### so overall recall score will be considered for good model. Logistics Regression will be chosen in this case.\n- As we can see that Train Recall score of Random forest is better than Logistics Regression but Test Recall score of Logistics Regression is better as compared to Random forest. Difference in between Train and test score is less for Logistics Regression which seems model is stable.","metadata":{}},{"cell_type":"markdown","source":"- Build another model with the main objective of identifying important predictor attributes which help the business understand indicators of churn. \n- A good choice to identify important variables is a logistic regression model or a model from the tree family. \n- In case of logistic regression, make sure to handle multi-collinearity.","metadata":{}},{"cell_type":"markdown","source":"### Recommend strategies to manage customer churn - Random Forest","metadata":{}},{"cell_type":"markdown","source":"**Fitting the final model with the best parameters obtained**","metadata":{}},{"cell_type":"code","source":"# model with the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=10,\n                             min_samples_leaf=100, \n                             min_samples_split=100,\n                             max_features=5,\n                             n_estimators=80)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit\nrfc.fit(X_train_smo,y_train_smo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nimpo_features = pd.Series(rfc.feature_importances_, index=X.columns)\nimpo_features.nlargest((25)).sort_values().plot(kind='barh', align='center')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - We could see from above plot that almost all the features of top 25 most important features are from action phase that is   August month. Hence we need to focus on these features to identify high value customers who may churn in future. \n - Lets look into some of the features","metadata":{}},{"cell_type":"markdown","source":"1. loc_ic_mou_8 : Local Incoming calls Minutes of usage - voice calls in 8th Month\n2. total_rech_amt_8 : Total recharge amount in 8th month\n3. loc_ic_t2m_mou_8 : Local Incoming calls Operator T to other operator mobile Minutes of usage - voice calls in 8th Month\n4. loc_og_mou_8 : Local Outgoing calls mobile Minutes of usage - voice calls in 8th Month\n5. max_rech_amt_8 : Max recharge amount in 8th month\n6. roam_ic_mou_8 : Roaming incoming calls Minutes of usage - voice calls in 8th Month\n7. last_day_rch_amt_8 : Last recharge amount in 8th month.\n8. total_month_rech_8 : Total recharge amount in 8th month\n9. total_ic_mou_8 : Total incoming calls Minutes of usage - voice calls in 8th Month\n10. total_og_mou_8 : Total Outgoing calls mobile Minutes of usage - voice calls in 8th Month\n11. loc_og_t2t_mou_8 : Local Outgoing calls within same operator mobile Minutes of usage - voice calls in 8th Month\n12. roam_og_mou_8 : Roaming outgoing calls Minutes of usage - voice calls in 8th Month\n13. offnet_mou_8 : All kind of calls outside the operator T network Minutes of usage - voice calls in 8th Month\n14. total_rech_num_data_8 : total number of data recharges done in the month 8\n15. loc_ic_t2t_mou_8: Local Incoming calls within same operator mobile Minutes of usage - voice calls in 8th Month","metadata":{}},{"cell_type":"markdown","source":"### Recommend strategies and Suggestions:","metadata":{}},{"cell_type":"markdown","source":"- Important factors that Telecom compnay should monitor is already mentioned above.\n\n- Number of Incoming and outgoing calls from a mobile number in particular month by customer. If number of calls starts reducing then it may be sign of customer trying to switch from one netwrok to another network or he is has already switched and using  current netwrok for few days.\n\n- Recharge amount is very important factor to notice if it starts reducing month by month then it need to be looked as cutomer may not be happy with the services he is getting that is why he started recharging with less amount.\n\n- If data usage starts decreasing and in august month it is minimal then it shows customer is not getting good spped of internet.\n\n- If internet speed that customer is getting is good, customer will finsh data soon and recharge it again but if network is poor and speed is not good then customer will not be able to finish it and will not recharge it multiple times. so need to look into areas where mnetwork is poor and customer care is receiving complaints multiple times.\n\n- if all kinds of call and data usage reduces then it is serious concern as customer may be planning to churn and just timepassing for few more days. so company need to look into these ares.\n\n- if customer is using the services for incoming calls only and has stopped using outgoing calls then he is finding the services very costly and may switch to network where incoming and outgoing services are in reasonable rate.","metadata":{}}]}